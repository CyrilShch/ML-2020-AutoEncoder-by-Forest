{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_trainMNIST_testOMNIGLOT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN-q7p92saLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "# Torchvision\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak3w61SEsiD0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca3cd555-f5e7-45d1-93a2-d4fbf758b406"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "dataset = torchvision.datasets.Omniglot(\n",
        "    root=\"./data\", download=True, transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "label=[]\n",
        "\n",
        "for i in range(0,len(dataset)):\n",
        "  if i==0:\n",
        "    image, _ = dataset[i]\n",
        "    label.append(dataset[i][1])\n",
        "    \n",
        "  elif dataset[i][1]!=label[-1]:\n",
        "    image=torch.cat((image, dataset[i][0]), dim=0)\n",
        "    label.append(dataset[i][1])\n",
        "\n",
        "label0=[]\n",
        "for i in range(0, 1000):\n",
        "  if i==0:\n",
        "    image=torch.cat((image, dataset[i+1][0]), dim=0)\n",
        "    label0.append(dataset[i+1][1])\n",
        "  elif dataset[i+1][1]!=label0[-1]:\n",
        "    image=torch.cat((image, dataset[i+1][0]), dim=0)\n",
        "    label0.append(dataset[i+1][1])\n",
        "  if len(label0)>=1000-len(label):\n",
        "    break\n",
        "\n",
        "\n",
        "x_test=image.numpy()\n",
        "label=np.array(label)\n",
        "label0=np.array(label0)\n",
        "y_test=np.concatenate((label, label0),axis=None)\n",
        "\n",
        "import cv2\n",
        "x_test_res=[]\n",
        "for i in range(0,len(x_test)):\n",
        "  x_test_res.append(cv2.resize(x_test[i], (28, 28)))\n",
        "x_test_res=np.array(x_test_res)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKWuuIFvtuP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f45f891-a762-4e5c-c4d5-f831e9a243ca"
      },
      "source": [
        "x_test_res.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCVoFcC0sq8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = (x_test_res * 255).astype('int')\n",
        "x_test = 255 - x_test\n",
        "x_test = x_test.reshape((x_test.shape[0], 1, 28, 28))\n",
        "x_test = x_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRKLRWbfswBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f7a4649c-207d-4613-dd79-3c236ac5cdc3"
      },
      "source": [
        "plt.imshow(x_test[1].reshape(28, 28), cmap = 'gray')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3e4d3bb1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4cRdCFUvMi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "79b5f800-efbe-431b-b831-e33a023541aa"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (_, _) = mnist.load_data()\n",
        "img_rows, img_cols = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "# x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "print(\"[mnist] x_train.shape={}, y_train.shape={}\".format(x_train.shape, y_train.shape))\n",
        "print(\"[omniglot] x_test.shape={}, y_test.shape={}\".format(x_test.shape, y_test.shape))\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainloader_mnist = torch.utils.data.DataLoader(x_train, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testloader_mnist = torch.utils.data.DataLoader(x_test, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "[mnist] x_train.shape=(60000, 1, 28, 28), y_train.shape=(60000,)\n",
            "[omniglot] x_test.shape=(1000, 1, 28, 28), y_test.shape=(1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nda1A5Wnvsic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6c90b0f4-7303-4ac4-b02a-a9d292c1b328"
      },
      "source": [
        "plt.imshow(x_test[0].reshape(28, 28), cmap = 'gray')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3dbe3bcc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiRB2IEkCDuQBGEHkvg/AS7gHN1BNxAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8UfgymEvwQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, net, criterion, optimizer, trainload, testloader, scheduler=None, verbose=True):\n",
        "    start = time.time()\n",
        "    net.to(device)\n",
        " \n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        net.train()\n",
        "        for i, inputs_mnist in enumerate(trainload):\n",
        "          \n",
        "            img = Variable(inputs_mnist).to(device)\n",
        "            _, outputs_mnist = net(img)\n",
        "            loss = criterion(outputs_mnist, img)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.data\n",
        "            \n",
        "            if verbose and i % 900 == 899:\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 900))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        running_loss_val = 0.0\n",
        "        net.eval()\n",
        "        for i, data in enumerate(testloader):\n",
        "\n",
        "            images = data  \n",
        "            images = Variable(images).to(device)\n",
        "            decoded_imgs = net(images)[1]\n",
        "            loss_val = criterion(decoded_imgs, images)\n",
        "            running_loss_val += loss_val.data\n",
        "\n",
        "            if verbose and i % 15 == 14:\n",
        "                print('[%d, %5d] val_loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss_val / 15))\n",
        "                running_loss_val = 0.0\n",
        "        print(\"============================\")\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Time cost for training (in seconds): {end-start}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDr3pk8Mv8Nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Autoencoder3(nn.Module):\n",
        "    def __init__(self,hid):\n",
        "        super(Autoencoder3, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "                        nn.Conv2d(1, 4, kernel_size=5),\n",
        "                        nn.BatchNorm2d(4),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "                        nn.Conv2d(4, 12, kernel_size=5),\n",
        "                        nn.BatchNorm2d(12),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "\n",
        "                        nn.Conv2d(12, 16,kernel_size=5),\n",
        "                        nn.BatchNorm2d(16),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "                        nn.Conv2d(16,28, kernel_size=5),\n",
        "                        nn.BatchNorm2d(28),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "                        nn.Conv2d(28, 36, kernel_size=5),\n",
        "                        nn.BatchNorm2d(36),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "                        nn.Conv2d(36, hid, kernel_size=5),\n",
        "                        nn.BatchNorm2d(hid),\n",
        "                        nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "                        nn.ConvTranspose2d(hid, 36, kernel_size=5),\n",
        "                        nn.BatchNorm2d(36),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "                        nn.ConvTranspose2d(36, 28, kernel_size=5),\n",
        "                        nn.BatchNorm2d(28),\n",
        "                        nn.ReLU(True),\n",
        " \n",
        "                        #nn.MaxUnpool2d(kernel_size=2),\n",
        "                        nn.ConvTranspose2d(28, 16, kernel_size=5),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "                        nn.ConvTranspose2d(16, 12, kernel_size=5),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "                        nn.ConvTranspose2d(12, 4, kernel_size=5),\n",
        "                        nn.ReLU(True),\n",
        "\n",
        "                        #nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
        "                        nn.ConvTranspose2d(4, 1, kernel_size=5),\n",
        "                        nn.ReLU(True))\n",
        "\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x_out = self.decoder(self.encoder(x))\n",
        "        \n",
        "        return _, x_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOjF1WNEwCke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import device as device_\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net = Autoencoder3(72).to(device)\n",
        "optimizer2 = torch.optim.Adam(net.parameters(), weight_decay=1e-5)\n",
        "epochs = 200\n",
        "criterion = nn.MSELoss().to(device)\n",
        "\n",
        "scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer2, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dAgTmprwEY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c5f5fab-061d-488d-c2e8-5f51896a1e07"
      },
      "source": [
        "train(epochs, net, criterion, optimizer2, trainloader_mnist, testloader_mnist,  scheduler=scheduler2, verbose=True)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   900] loss: 1279.446\n",
            "[1,    15] val_loss: 1675.095\n",
            "============================\n",
            "[2,   900] loss: 482.286\n",
            "[2,    15] val_loss: 1410.392\n",
            "============================\n",
            "[3,   900] loss: 367.128\n",
            "[3,    15] val_loss: 1297.380\n",
            "============================\n",
            "[4,   900] loss: 311.278\n",
            "[4,    15] val_loss: 1165.155\n",
            "============================\n",
            "[5,   900] loss: 277.811\n",
            "[5,    15] val_loss: 1089.933\n",
            "============================\n",
            "[6,   900] loss: 254.593\n",
            "[6,    15] val_loss: 1043.115\n",
            "============================\n",
            "[7,   900] loss: 232.933\n",
            "[7,    15] val_loss: 999.239\n",
            "============================\n",
            "[8,   900] loss: 218.254\n",
            "[8,    15] val_loss: 957.869\n",
            "============================\n",
            "[9,   900] loss: 203.673\n",
            "[9,    15] val_loss: 899.810\n",
            "============================\n",
            "[10,   900] loss: 193.073\n",
            "[10,    15] val_loss: 846.203\n",
            "============================\n",
            "[11,   900] loss: 181.802\n",
            "[11,    15] val_loss: 833.561\n",
            "============================\n",
            "[12,   900] loss: 172.161\n",
            "[12,    15] val_loss: 806.022\n",
            "============================\n",
            "[13,   900] loss: 165.806\n",
            "[13,    15] val_loss: 777.030\n",
            "============================\n",
            "[14,   900] loss: 157.891\n",
            "[14,    15] val_loss: 750.142\n",
            "============================\n",
            "[15,   900] loss: 151.867\n",
            "[15,    15] val_loss: 721.983\n",
            "============================\n",
            "[16,   900] loss: 145.503\n",
            "[16,    15] val_loss: 696.835\n",
            "============================\n",
            "[17,   900] loss: 139.485\n",
            "[17,    15] val_loss: 674.431\n",
            "============================\n",
            "[18,   900] loss: 135.211\n",
            "[18,    15] val_loss: 656.744\n",
            "============================\n",
            "[19,   900] loss: 130.850\n",
            "[19,    15] val_loss: 641.756\n",
            "============================\n",
            "[20,   900] loss: 126.251\n",
            "[20,    15] val_loss: 635.814\n",
            "============================\n",
            "[21,   900] loss: 122.576\n",
            "[21,    15] val_loss: 614.309\n",
            "============================\n",
            "[22,   900] loss: 118.452\n",
            "[22,    15] val_loss: 595.685\n",
            "============================\n",
            "[23,   900] loss: 115.561\n",
            "[23,    15] val_loss: 594.192\n",
            "============================\n",
            "[24,   900] loss: 113.527\n",
            "[24,    15] val_loss: 568.636\n",
            "============================\n",
            "[25,   900] loss: 110.278\n",
            "[25,    15] val_loss: 578.873\n",
            "============================\n",
            "[26,   900] loss: 107.273\n",
            "[26,    15] val_loss: 558.795\n",
            "============================\n",
            "[27,   900] loss: 105.008\n",
            "[27,    15] val_loss: 550.971\n",
            "============================\n",
            "[28,   900] loss: 102.411\n",
            "[28,    15] val_loss: 541.427\n",
            "============================\n",
            "[29,   900] loss: 100.416\n",
            "[29,    15] val_loss: 528.302\n",
            "============================\n",
            "[30,   900] loss: 97.474\n",
            "[30,    15] val_loss: 529.428\n",
            "============================\n",
            "[31,   900] loss: 95.855\n",
            "[31,    15] val_loss: 509.053\n",
            "============================\n",
            "[32,   900] loss: 94.284\n",
            "[32,    15] val_loss: 503.282\n",
            "============================\n",
            "[33,   900] loss: 92.152\n",
            "[33,    15] val_loss: 493.374\n",
            "============================\n",
            "[34,   900] loss: 90.268\n",
            "[34,    15] val_loss: 496.277\n",
            "============================\n",
            "[35,   900] loss: 89.368\n",
            "[35,    15] val_loss: 486.971\n",
            "============================\n",
            "[36,   900] loss: 87.472\n",
            "[36,    15] val_loss: 480.830\n",
            "============================\n",
            "[37,   900] loss: 86.019\n",
            "[37,    15] val_loss: 478.403\n",
            "============================\n",
            "[38,   900] loss: 84.279\n",
            "[38,    15] val_loss: 465.596\n",
            "============================\n",
            "[39,   900] loss: 82.773\n",
            "[39,    15] val_loss: 466.239\n",
            "============================\n",
            "[40,   900] loss: 81.488\n",
            "[40,    15] val_loss: 453.471\n",
            "============================\n",
            "[41,   900] loss: 79.938\n",
            "[41,    15] val_loss: 456.084\n",
            "============================\n",
            "[42,   900] loss: 79.043\n",
            "[42,    15] val_loss: 450.303\n",
            "============================\n",
            "[43,   900] loss: 77.755\n",
            "[43,    15] val_loss: 443.222\n",
            "============================\n",
            "[44,   900] loss: 76.979\n",
            "[44,    15] val_loss: 439.268\n",
            "============================\n",
            "[45,   900] loss: 75.473\n",
            "[45,    15] val_loss: 438.147\n",
            "============================\n",
            "[46,   900] loss: 74.119\n",
            "[46,    15] val_loss: 432.115\n",
            "============================\n",
            "[47,   900] loss: 73.335\n",
            "[47,    15] val_loss: 418.145\n",
            "============================\n",
            "[48,   900] loss: 72.050\n",
            "[48,    15] val_loss: 414.204\n",
            "============================\n",
            "[49,   900] loss: 71.024\n",
            "[49,    15] val_loss: 406.242\n",
            "============================\n",
            "[50,   900] loss: 70.220\n",
            "[50,    15] val_loss: 396.144\n",
            "============================\n",
            "[51,   900] loss: 69.398\n",
            "[51,    15] val_loss: 396.443\n",
            "============================\n",
            "[52,   900] loss: 68.695\n",
            "[52,    15] val_loss: 393.392\n",
            "============================\n",
            "[53,   900] loss: 67.377\n",
            "[53,    15] val_loss: 400.530\n",
            "============================\n",
            "[54,   900] loss: 66.242\n",
            "[54,    15] val_loss: 386.115\n",
            "============================\n",
            "[55,   900] loss: 65.752\n",
            "[55,    15] val_loss: 387.733\n",
            "============================\n",
            "[56,   900] loss: 64.824\n",
            "[56,    15] val_loss: 375.570\n",
            "============================\n",
            "[57,   900] loss: 63.748\n",
            "[57,    15] val_loss: 377.539\n",
            "============================\n",
            "[58,   900] loss: 62.794\n",
            "[58,    15] val_loss: 370.125\n",
            "============================\n",
            "[59,   900] loss: 62.241\n",
            "[59,    15] val_loss: 362.778\n",
            "============================\n",
            "[60,   900] loss: 61.868\n",
            "[60,    15] val_loss: 367.855\n",
            "============================\n",
            "[61,   900] loss: 61.047\n",
            "[61,    15] val_loss: 356.190\n",
            "============================\n",
            "[62,   900] loss: 60.119\n",
            "[62,    15] val_loss: 361.014\n",
            "============================\n",
            "[63,   900] loss: 59.565\n",
            "[63,    15] val_loss: 370.219\n",
            "============================\n",
            "[64,   900] loss: 59.333\n",
            "[64,    15] val_loss: 350.305\n",
            "============================\n",
            "[65,   900] loss: 58.333\n",
            "[65,    15] val_loss: 353.387\n",
            "============================\n",
            "[66,   900] loss: 57.700\n",
            "[66,    15] val_loss: 343.165\n",
            "============================\n",
            "[67,   900] loss: 57.021\n",
            "[67,    15] val_loss: 339.017\n",
            "============================\n",
            "[68,   900] loss: 56.659\n",
            "[68,    15] val_loss: 351.127\n",
            "============================\n",
            "[69,   900] loss: 55.833\n",
            "[69,    15] val_loss: 335.416\n",
            "============================\n",
            "[70,   900] loss: 55.393\n",
            "[70,    15] val_loss: 341.722\n",
            "============================\n",
            "[71,   900] loss: 55.095\n",
            "[71,    15] val_loss: 335.839\n",
            "============================\n",
            "[72,   900] loss: 54.045\n",
            "[72,    15] val_loss: 327.527\n",
            "============================\n",
            "[73,   900] loss: 54.202\n",
            "[73,    15] val_loss: 329.850\n",
            "============================\n",
            "[74,   900] loss: 53.559\n",
            "[74,    15] val_loss: 321.211\n",
            "============================\n",
            "[75,   900] loss: 52.631\n",
            "[75,    15] val_loss: 319.302\n",
            "============================\n",
            "[76,   900] loss: 52.349\n",
            "[76,    15] val_loss: 315.062\n",
            "============================\n",
            "[77,   900] loss: 51.954\n",
            "[77,    15] val_loss: 312.498\n",
            "============================\n",
            "[78,   900] loss: 51.220\n",
            "[78,    15] val_loss: 313.403\n",
            "============================\n",
            "[79,   900] loss: 50.850\n",
            "[79,    15] val_loss: 319.030\n",
            "============================\n",
            "[80,   900] loss: 50.345\n",
            "[80,    15] val_loss: 312.931\n",
            "============================\n",
            "[81,   900] loss: 49.998\n",
            "[81,    15] val_loss: 304.065\n",
            "============================\n",
            "[82,   900] loss: 49.456\n",
            "[82,    15] val_loss: 303.009\n",
            "============================\n",
            "[83,   900] loss: 49.094\n",
            "[83,    15] val_loss: 298.746\n",
            "============================\n",
            "[84,   900] loss: 48.587\n",
            "[84,    15] val_loss: 302.917\n",
            "============================\n",
            "[85,   900] loss: 48.454\n",
            "[85,    15] val_loss: 297.170\n",
            "============================\n",
            "[86,   900] loss: 47.763\n",
            "[86,    15] val_loss: 297.229\n",
            "============================\n",
            "[87,   900] loss: 47.660\n",
            "[87,    15] val_loss: 293.246\n",
            "============================\n",
            "[88,   900] loss: 47.323\n",
            "[88,    15] val_loss: 295.545\n",
            "============================\n",
            "[89,   900] loss: 46.903\n",
            "[89,    15] val_loss: 288.380\n",
            "============================\n",
            "[90,   900] loss: 46.323\n",
            "[90,    15] val_loss: 288.084\n",
            "============================\n",
            "[91,   900] loss: 45.978\n",
            "[91,    15] val_loss: 289.199\n",
            "============================\n",
            "[92,   900] loss: 45.691\n",
            "[92,    15] val_loss: 289.113\n",
            "============================\n",
            "[93,   900] loss: 45.481\n",
            "[93,    15] val_loss: 280.927\n",
            "============================\n",
            "[94,   900] loss: 45.148\n",
            "[94,    15] val_loss: 282.248\n",
            "============================\n",
            "[95,   900] loss: 44.670\n",
            "[95,    15] val_loss: 283.333\n",
            "============================\n",
            "[96,   900] loss: 44.350\n",
            "[96,    15] val_loss: 282.885\n",
            "============================\n",
            "[97,   900] loss: 44.181\n",
            "[97,    15] val_loss: 279.256\n",
            "============================\n",
            "[98,   900] loss: 43.822\n",
            "[98,    15] val_loss: 276.641\n",
            "============================\n",
            "[99,   900] loss: 43.477\n",
            "[99,    15] val_loss: 273.530\n",
            "============================\n",
            "[100,   900] loss: 43.254\n",
            "[100,    15] val_loss: 272.994\n",
            "============================\n",
            "[101,   900] loss: 43.001\n",
            "[101,    15] val_loss: 275.455\n",
            "============================\n",
            "[102,   900] loss: 42.497\n",
            "[102,    15] val_loss: 272.955\n",
            "============================\n",
            "[103,   900] loss: 42.402\n",
            "[103,    15] val_loss: 268.586\n",
            "============================\n",
            "[104,   900] loss: 42.175\n",
            "[104,    15] val_loss: 270.378\n",
            "============================\n",
            "[105,   900] loss: 41.864\n",
            "[105,    15] val_loss: 267.631\n",
            "============================\n",
            "[106,   900] loss: 41.553\n",
            "[106,    15] val_loss: 266.046\n",
            "============================\n",
            "[107,   900] loss: 41.295\n",
            "[107,    15] val_loss: 262.544\n",
            "============================\n",
            "[108,   900] loss: 41.180\n",
            "[108,    15] val_loss: 259.428\n",
            "============================\n",
            "[109,   900] loss: 40.759\n",
            "[109,    15] val_loss: 262.099\n",
            "============================\n",
            "[110,   900] loss: 40.502\n",
            "[110,    15] val_loss: 260.066\n",
            "============================\n",
            "[111,   900] loss: 40.322\n",
            "[111,    15] val_loss: 260.343\n",
            "============================\n",
            "[112,   900] loss: 40.076\n",
            "[112,    15] val_loss: 256.278\n",
            "============================\n",
            "[113,   900] loss: 39.741\n",
            "[113,    15] val_loss: 256.439\n",
            "============================\n",
            "[114,   900] loss: 39.615\n",
            "[114,    15] val_loss: 257.805\n",
            "============================\n",
            "[115,   900] loss: 39.457\n",
            "[115,    15] val_loss: 255.561\n",
            "============================\n",
            "[116,   900] loss: 39.293\n",
            "[116,    15] val_loss: 252.173\n",
            "============================\n",
            "[117,   900] loss: 38.886\n",
            "[117,    15] val_loss: 250.826\n",
            "============================\n",
            "[118,   900] loss: 38.653\n",
            "[118,    15] val_loss: 250.449\n",
            "============================\n",
            "[119,   900] loss: 38.466\n",
            "[119,    15] val_loss: 249.538\n",
            "============================\n",
            "[120,   900] loss: 38.289\n",
            "[120,    15] val_loss: 249.216\n",
            "============================\n",
            "[121,   900] loss: 38.139\n",
            "[121,    15] val_loss: 245.583\n",
            "============================\n",
            "[122,   900] loss: 37.949\n",
            "[122,    15] val_loss: 248.032\n",
            "============================\n",
            "[123,   900] loss: 37.715\n",
            "[123,    15] val_loss: 246.045\n",
            "============================\n",
            "[124,   900] loss: 37.560\n",
            "[124,    15] val_loss: 242.167\n",
            "============================\n",
            "[125,   900] loss: 37.484\n",
            "[125,    15] val_loss: 243.682\n",
            "============================\n",
            "[126,   900] loss: 37.253\n",
            "[126,    15] val_loss: 240.559\n",
            "============================\n",
            "[127,   900] loss: 37.186\n",
            "[127,    15] val_loss: 241.928\n",
            "============================\n",
            "[128,   900] loss: 36.902\n",
            "[128,    15] val_loss: 241.118\n",
            "============================\n",
            "[129,   900] loss: 36.837\n",
            "[129,    15] val_loss: 238.891\n",
            "============================\n",
            "[130,   900] loss: 36.602\n",
            "[130,    15] val_loss: 241.566\n",
            "============================\n",
            "[131,   900] loss: 36.322\n",
            "[131,    15] val_loss: 239.520\n",
            "============================\n",
            "[132,   900] loss: 36.227\n",
            "[132,    15] val_loss: 236.355\n",
            "============================\n",
            "[133,   900] loss: 36.166\n",
            "[133,    15] val_loss: 235.682\n",
            "============================\n",
            "[134,   900] loss: 35.950\n",
            "[134,    15] val_loss: 236.070\n",
            "============================\n",
            "[135,   900] loss: 35.795\n",
            "[135,    15] val_loss: 233.009\n",
            "============================\n",
            "[136,   900] loss: 35.519\n",
            "[136,    15] val_loss: 232.987\n",
            "============================\n",
            "[137,   900] loss: 35.588\n",
            "[137,    15] val_loss: 233.419\n",
            "============================\n",
            "[138,   900] loss: 35.331\n",
            "[138,    15] val_loss: 231.168\n",
            "============================\n",
            "[139,   900] loss: 35.259\n",
            "[139,    15] val_loss: 231.967\n",
            "============================\n",
            "[140,   900] loss: 35.115\n",
            "[140,    15] val_loss: 231.961\n",
            "============================\n",
            "[141,   900] loss: 34.993\n",
            "[141,    15] val_loss: 231.233\n",
            "============================\n",
            "[142,   900] loss: 34.832\n",
            "[142,    15] val_loss: 226.879\n",
            "============================\n",
            "[143,   900] loss: 34.704\n",
            "[143,    15] val_loss: 228.566\n",
            "============================\n",
            "[144,   900] loss: 34.578\n",
            "[144,    15] val_loss: 227.884\n",
            "============================\n",
            "[145,   900] loss: 34.370\n",
            "[145,    15] val_loss: 226.218\n",
            "============================\n",
            "[146,   900] loss: 34.329\n",
            "[146,    15] val_loss: 228.155\n",
            "============================\n",
            "[147,   900] loss: 34.176\n",
            "[147,    15] val_loss: 225.445\n",
            "============================\n",
            "[148,   900] loss: 34.196\n",
            "[148,    15] val_loss: 224.953\n",
            "============================\n",
            "[149,   900] loss: 34.046\n",
            "[149,    15] val_loss: 224.219\n",
            "============================\n",
            "[150,   900] loss: 33.967\n",
            "[150,    15] val_loss: 224.128\n",
            "============================\n",
            "[151,   900] loss: 33.768\n",
            "[151,    15] val_loss: 225.190\n",
            "============================\n",
            "[152,   900] loss: 33.712\n",
            "[152,    15] val_loss: 222.815\n",
            "============================\n",
            "[153,   900] loss: 33.601\n",
            "[153,    15] val_loss: 222.501\n",
            "============================\n",
            "[154,   900] loss: 33.422\n",
            "[154,    15] val_loss: 222.095\n",
            "============================\n",
            "[155,   900] loss: 33.498\n",
            "[155,    15] val_loss: 221.423\n",
            "============================\n",
            "[156,   900] loss: 33.302\n",
            "[156,    15] val_loss: 222.417\n",
            "============================\n",
            "[157,   900] loss: 33.254\n",
            "[157,    15] val_loss: 220.655\n",
            "============================\n",
            "[158,   900] loss: 33.096\n",
            "[158,    15] val_loss: 220.662\n",
            "============================\n",
            "[159,   900] loss: 33.150\n",
            "[159,    15] val_loss: 221.470\n",
            "============================\n",
            "[160,   900] loss: 33.029\n",
            "[160,    15] val_loss: 219.665\n",
            "============================\n",
            "[161,   900] loss: 32.929\n",
            "[161,    15] val_loss: 219.739\n",
            "============================\n",
            "[162,   900] loss: 32.836\n",
            "[162,    15] val_loss: 220.304\n",
            "============================\n",
            "[163,   900] loss: 32.786\n",
            "[163,    15] val_loss: 217.591\n",
            "============================\n",
            "[164,   900] loss: 32.767\n",
            "[164,    15] val_loss: 218.230\n",
            "============================\n",
            "[165,   900] loss: 32.567\n",
            "[165,    15] val_loss: 216.845\n",
            "============================\n",
            "[166,   900] loss: 32.545\n",
            "[166,    15] val_loss: 218.048\n",
            "============================\n",
            "[167,   900] loss: 32.451\n",
            "[167,    15] val_loss: 217.412\n",
            "============================\n",
            "[168,   900] loss: 32.454\n",
            "[168,    15] val_loss: 216.763\n",
            "============================\n",
            "[169,   900] loss: 32.368\n",
            "[169,    15] val_loss: 215.956\n",
            "============================\n",
            "[170,   900] loss: 32.302\n",
            "[170,    15] val_loss: 217.485\n",
            "============================\n",
            "[171,   900] loss: 32.135\n",
            "[171,    15] val_loss: 216.480\n",
            "============================\n",
            "[172,   900] loss: 32.211\n",
            "[172,    15] val_loss: 216.285\n",
            "============================\n",
            "[173,   900] loss: 32.116\n",
            "[173,    15] val_loss: 215.598\n",
            "============================\n",
            "[174,   900] loss: 32.137\n",
            "[174,    15] val_loss: 215.895\n",
            "============================\n",
            "[175,   900] loss: 32.027\n",
            "[175,    15] val_loss: 214.625\n",
            "============================\n",
            "[176,   900] loss: 31.956\n",
            "[176,    15] val_loss: 215.406\n",
            "============================\n",
            "[177,   900] loss: 31.961\n",
            "[177,    15] val_loss: 215.186\n",
            "============================\n",
            "[178,   900] loss: 31.914\n",
            "[178,    15] val_loss: 213.672\n",
            "============================\n",
            "[179,   900] loss: 31.766\n",
            "[179,    15] val_loss: 213.952\n",
            "============================\n",
            "[180,   900] loss: 31.892\n",
            "[180,    15] val_loss: 214.088\n",
            "============================\n",
            "[181,   900] loss: 31.811\n",
            "[181,    15] val_loss: 214.664\n",
            "============================\n",
            "[182,   900] loss: 31.790\n",
            "[182,    15] val_loss: 213.966\n",
            "============================\n",
            "[183,   900] loss: 31.679\n",
            "[183,    15] val_loss: 213.622\n",
            "============================\n",
            "[184,   900] loss: 31.714\n",
            "[184,    15] val_loss: 213.413\n",
            "============================\n",
            "[185,   900] loss: 31.682\n",
            "[185,    15] val_loss: 213.883\n",
            "============================\n",
            "[186,   900] loss: 31.736\n",
            "[186,    15] val_loss: 213.519\n",
            "============================\n",
            "[187,   900] loss: 31.677\n",
            "[187,    15] val_loss: 213.353\n",
            "============================\n",
            "[188,   900] loss: 31.569\n",
            "[188,    15] val_loss: 213.774\n",
            "============================\n",
            "[189,   900] loss: 31.494\n",
            "[189,    15] val_loss: 212.970\n",
            "============================\n",
            "[190,   900] loss: 31.475\n",
            "[190,    15] val_loss: 213.234\n",
            "============================\n",
            "[191,   900] loss: 31.543\n",
            "[191,    15] val_loss: 213.225\n",
            "============================\n",
            "[192,   900] loss: 31.491\n",
            "[192,    15] val_loss: 212.554\n",
            "============================\n",
            "[193,   900] loss: 31.427\n",
            "[193,    15] val_loss: 213.402\n",
            "============================\n",
            "[194,   900] loss: 31.473\n",
            "[194,    15] val_loss: 213.108\n",
            "============================\n",
            "[195,   900] loss: 31.446\n",
            "[195,    15] val_loss: 213.683\n",
            "============================\n",
            "[196,   900] loss: 31.480\n",
            "[196,    15] val_loss: 213.132\n",
            "============================\n",
            "[197,   900] loss: 31.491\n",
            "[197,    15] val_loss: 213.545\n",
            "============================\n",
            "[198,   900] loss: 31.406\n",
            "[198,    15] val_loss: 213.366\n",
            "============================\n",
            "[199,   900] loss: 31.420\n",
            "[199,    15] val_loss: 213.890\n",
            "============================\n",
            "[200,   900] loss: 31.421\n",
            "[200,    15] val_loss: 212.947\n",
            "============================\n",
            "Time cost for training (in seconds): 2255.5951240062714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbYDzDV8wnBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "running_loss_val = 0.0\n",
        "i = 0\n",
        "reconstruction_time=[]\n",
        "decoded_pics = []\n",
        "testloader = torch.utils.data.DataLoader(x_test, batch_size=1,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images = data  \n",
        "        images = Variable(images).to(device)\n",
        "        i += 1\n",
        "        encoded_imgs = net.encoder(images)\n",
        "        start=time.time()\n",
        "        decoded_imgs = net.decoder(encoded_imgs)\n",
        "        end=time.time()\n",
        "        reconstruction_time.append(end-start)\n",
        "        decoded_pics.append(decoded_imgs)\n",
        "        loss_val = criterion(decoded_imgs, images)\n",
        "        running_loss_val += loss_val.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-D8kvhK5rk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e51c0678-8258-4e7c-e09a-dd82caf2221d"
      },
      "source": [
        "mean=np.mean(np.array(reconstruction_time))\n",
        "print(f'MSE of the tunned CNN AE on the 1000 test OMNIGLOT images trained on MNIST: {running_loss_val/i}')\n",
        "print(f\"Decoding time {mean} is measured in sample per seconds\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE of the tunned CNN AE on the 1000 test OMNIGLOT images trained on MNIST: 210.5996856689453\n",
            "Decoding time 0.0006261920928955078 is measured in sample per seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSgZSxfj5z6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c9707c4b-367f-4792-bd60-a47ca57a66b2"
      },
      "source": [
        "fig, ax = plt.subplots(ncols=10, nrows=2, figsize=(20, 5))\n",
        "\n",
        "for X_test in testloader_mnist:\n",
        "  for i in range(10):\n",
        "      im = X_test[i,:]\n",
        "      ax[0, i].imshow(im.numpy().reshape(28, 28).astype(int), cmap = 'gray');\n",
        "      ax[1, i].imshow(decoded_pics[i].cpu().numpy().reshape(28, 28).astype(int), cmap = 'gray')\n",
        "      ax[0, i].get_xaxis().set_ticks([])\n",
        "      ax[0, i].get_yaxis().set_ticks([])\n",
        "      ax[1, i].get_xaxis().set_ticks([])\n",
        "      ax[1, i].get_yaxis().set_ticks([])\n",
        "\n",
        "\n",
        "  break\n",
        "ax[0, 0].set_ylabel(f\"$Original$\",fontsize=18)\n",
        "ax[1, 0].set_ylabel(f\"$CNN_2$\",fontsize=18)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '$CNN_2$')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAAEECAYAAABZWu9JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x360 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnPpMZSd6F0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}